These files facilitate the generation and use of input data for machine learning for SRW. They include:

{{ shFileName }}: a shell script that generates input data
{{ pyFileName }}: a python script that runs SRW using the generated data
{{ ymlFileName }}: a YML configuration file for rsopt
{{ readmeFileName }}: this README

{% if hasLibFiles %}
Also included are auxiliary files required for the specific SRW simulation:
{{ libFiles }}
{% endif %}

To generate the data, run
    'bash {{ shFileName }}'.

This stores the data in the numpy file
    H_sample_{{ fileBase }}_history_length={{ totalSamples }}_evals={{ totalSamples }}_workers={{ numWorkers }}.npy

The script will prompt you to then run SRW with that data. To do so later, run
    'python {{ pyFileName }} rsopt_run H_sample_{{ fileBase }}_history_length={{ totalSamples }}_evals={{ totalSamples }}_workers={{ numWorkers }}.npy'.

Please note:
    - rsopt does the job of running SRW with the stored input data
    - If the machine running SRW has N cores, rsopt will attempt execute in parallel on N - 1 cores
    - SRW will run {{ totalSamples }} times; use caution

The final consolidated output data for use in ML is in two numpy files under the "datasets" folder. Each is an array
with {{ totalSamples }} entries, one for each run.  
    - beam_intensities_{{ rsOptResFileInfix }}{{ totalSamples }}_runs.npy: propagated single-e intensity distribution vs horizontal and vertical position
    - parameters_{{ rsOptResFileInfix }}{{ totalSamples }}_runs.npy: the inputs used for each run


Intermediate output is also available (note the files may be re-used by workers):

data_files/res_int_pr_se_[n].dat:
    propagated single-e intensity distribution vs horizontal and vertical position

beams/beam_[n].npy:
    same information as in "data_files", but in numpy format

parameters/values_[n].npy:
    values of the parameters

