import numpy as np
import keras.losses
from keras.callbacks import CSVLogger
from pykern.pkcollections import PKDict
{% if image_data %}

model.compile(
    optimizer='adam',
    loss={{ neuralNet_losses }},
    metrics=['accuracy'],
)
try:
    history = model.fit(
        split.train_gen,
        validation_data=split.val_gen,
        epochs={{ neuralNet_epochs }},
        callbacks=[CSVLogger('fit.csv')],
    )
except Exception as e:
    raise AssertionError(f"Model training failed due to:{e.message}")
{% else %}

model.compile(
    loss={{ neuralNet_losses }},
    optimizer='{{ neuralNet_optimizer }}'
)
model.fit(
    x=train[:, in_idx],
    y=train[:, out_idx],
    validation_data=(validate[:, in_idx], validate[:, out_idx]),
    batch_size={{ neuralNet_batch_size }},
    shuffle={% if neuralNet_shuffle == '1' %}True{% else %}False{% endif %},
    epochs={{ neuralNet_epochs }},
    verbose=False,
    callbacks=[CSVLogger('{{ fitCSVFile }}')],
)
testy = test[:, out_idx]
testx = test[:, in_idx]

{% endif %}

{% if image_data %}
def _domain_len(outputs_true):
    return len(numpy.unique(outputs_true))

def _prediction_shape(prediction, testy):
    return prediction.shape[-1] == _domain_len(testy)

def _classification_case(prediction, testy):
    return {{ discreteOutputs }} and _prediction_shape(prediction, testy)

def _predictions_final(predictions, testy):
    if _classification_case(predictions, testy) and not {{ imageToImage }}:
        return np.argmax(predictions, axis=1)
    if {{ imageToImage }}:
        f = []
        for prediction in predictions:
            f.append(numpy.expand_dims(prediction, axis=-1))
        return numpy.array(f).flatten()
    return predictions

def _predictions_initial(testx, testy):
    if split.test_gen.outScaler and not {{ imageToImage }}:
        return PKDict(
            p=split.test_gen.outScaler.inverse_transform(model.predict(testx)),
            testy = split.test_gen.outScaler.inverse_transform(
                numpy.array(testy).reshape(-1, split.test_gen.y_channels)
            )
        )
    p = model.predict(testx)
    # TODO (gurhar1133): get indices of best and worst images
    b = _best_worst_images(testy, p.reshape(*numpy.array(testy).shape))
    return PKDict(
        p=p,
        best=b.best,
        worst=b.worst,
        testy=_test_y(testy),
    )

def _test_y(testy):
    if split.test_gen.outScaler:
        testy = split.test_gen.outScaler.inverse_transform(
            numpy.array(testy).reshape(-1, split.test_gen.y_channels)
        )
    if {{ imageToImage }}:
        testy = numpy.array(testy).flatten()
    return testy


def _best_worst_images(y_true, predictions):
    loss_fn = {{ neuralNet_losses }}
    losses = []
    for i in range(len(y_true)):
        losses.append((i, float(loss_fn(y_true[i], predictions[i]))))
    losses.sort(key = lambda x: x[1])
    {# print(losses[:4], losses[-4:]) #}
    return PKDict(best=[x[0] for x in losses[:3]], worst=[x[0] for x in losses[-3:]])


o = _predictions_initial(testx, testy)
p = _predictions_final(o.p, o.testy)
print(o.best, o.worst)
{% else %}
p = model.predict(x=testx)
{% endif %}
model.save('{{ weightedFile }}')
np.save('{{ testFile }}', {% if image_data %}o.testy{% else %}testy{% endif %})
np.save('{{ predictFile }}', p)

