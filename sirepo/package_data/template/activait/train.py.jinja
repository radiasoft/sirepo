import numpy as np
import keras.losses
from keras.callbacks import CSVLogger
from pykern.pkcollections import PKDict
{% if image_data %}

model.compile(
    optimizer='adam',
    loss={{ neuralNet_losses }},
    metrics=['accuracy'],
)
try:
    history = model.fit(
        split.train_gen,
        validation_data=split.val_gen,
        epochs={{ neuralNet_epochs }},
        callbacks=[CSVLogger('fit.csv')],
    )
except Exception as e:
    raise AssertionError(f"Model training failed due to:{e.message}")
{% else %}

model.compile(
    loss={{ neuralNet_losses }},
    optimizer='{{ neuralNet_optimizer }}'
)
model.fit(
    x=train[:, in_idx],
    y=train[:, out_idx],
    validation_data=(validate[:, in_idx], validate[:, out_idx]),
    batch_size={{ neuralNet_batch_size }},
    shuffle={% if neuralNet_shuffle == '1' %}True{% else %}False{% endif %},
    epochs={{ neuralNet_epochs }},
    verbose=False,
    callbacks=[CSVLogger('{{ fitCSVFile }}')],
)
testy = test[:, out_idx]
testx = test[:, in_idx]

{% endif %}

{% if image_data %}
def _domain_len(outputs_true):
    return len(numpy.unique(outputs_true))

def _prediction_shape(prediction, testy):
    return prediction.shape[-1] == _domain_len(testy)

def _classification_case(prediction, testy):
    return {{ discreteOutputs }} and _prediction_shape(prediction, testy)

def _predictions_final(predictions, testy):
    if _classification_case(predictions, testy) and not {{ imageOut }}:
        return np.argmax(predictions, axis=1)
    if {{ imageOut }}:
        f = []
        for prediction in predictions:
            f.append(numpy.expand_dims(prediction, axis=-1))
        return numpy.array(f).flatten()
    return predictions

def _predictions_initial(testx, testy):
    if split.test_gen.outScaler and not {{ imageOut }}:
        return PKDict(
            p=split.test_gen.outScaler.inverse_transform(model.predict(testx)),
            testy = split.test_gen.outScaler.inverse_transform(
                numpy.array(testy).reshape(-1, split.test_gen.y_channels)
            )
        )
    p = model.predict(testx)
    {% if imageOut %}
    b = _best_worst_images(testy, p.reshape(*numpy.array(testy).shape))
    {% endif %}
    return PKDict(
        p=p,
        {% if imageOut %}
        best=b.best,
        worst=b.worst,
        {% endif %}
        testy=_test_y(testy),
    )

def _test_y(testy):
    if split.test_gen.outScaler:
        testy = split.test_gen.outScaler.inverse_transform(
            numpy.array(testy).reshape(-1, split.test_gen.y_channels)
        )
    if {{ imageOut }}:
        testy = numpy.array(testy).flatten()
    return testy


def _best_worst_images(y_true, predictions):
    loss_fn = {{ neuralNet_losses }}
    losses = []
    for i in range(len(y_true)):
        losses.append((i, float(loss_fn(y_true[i], predictions[i]))))
    losses.sort(key = lambda x: x[1])
    return PKDict(best=[x[0] for x in losses[:3]], worst=[x[0] for x in losses[-3:]])


o = _predictions_initial(testx, testy)
p = _predictions_final(o.p, o.testy)
{% else %}
p = model.predict(x=testx)
{% endif %}
model.save('{{ weightedFile }}')
np.save('{{ testFile }}', {% if image_data %}o.testy{% else %}testy{% endif %})
np.save('{{ predictFile }}', p)
{% if imageOut %}
np.save('{{ bestFile }}', o.best)
if len(testx.original_shape_x) > 2:
    c = 0
    original_x = numpy.array([])
    for batch in testx:
        for img in batch:
            c += 1
            original_x = np.append([original_x], [img])
    original_x = np.reshape(original_x, (c, *testx.original_shape_x[1:]))
    np.save('{{ originalImageInFile }}', original_x)
np.save('{{ worstFile }}', o.worst)
{% endif %}
