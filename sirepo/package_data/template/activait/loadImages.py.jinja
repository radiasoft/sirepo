import h5py
import numpy
from sklearn.model_selection import train_test_split
from pykern.pkcollections import PKDict
from sirepo.template.activait import SirepoHDF5ImageGenerator


{% if dataFile_inputsScaler != 'None' %}
from sklearn.preprocessing import {{ dataFile_inputsScaler }} as in_scaler_fn
{% endif %}
{% if dataFile_outputsScaler != 'None' and dataFile_outputsScaler != dataFile_inputsScaler %}
from sklearn.preprocessing import {{ dataFile_outputsScaler }} as out_scale_fn
{% endif %}

def _scale(values):
    # TODO(pjm): use selected input/output scalar on dataFile model
    return 2 * (values - numpy.min(values)) / (numpy.max(values) - numpy.min(values)) - 1.0


def _split_indices(indices_all):
    test_and_validate = (100 - {{partition_training}}) / 100
    validation_size = ((test_and_validate * 100) - {{partition_testing}}) / 100
    train_indices, tvx, _, tvy = train_test_split(
        indices_all, indices_all, test_size=test_and_validate, random_state=42, shuffle=False
    )
    test_indices, val_indices, _, _ = train_test_split(
        tvx, tvy, test_size=validation_size/test_and_validate, random_state=42, shuffle=False
    )
    return PKDict(
        train_indices=train_indices,
        validation_indices=val_indices,
        test_indices=test_indices,
    )


def image_train_val_test_split(
    **kwargs
):
    print("kwargs:", kwargs)
    with h5py.File(kwargs.get("src"), "r", libver="latest", swmr=True) as file:
        indices_all = numpy.arange(file[kwargs.get("X_key")].shape[0])
    s = _split_indices(indices_all)
    return PKDict(
        train_generator=SirepoHDF5ImageGenerator(
            indices=s.train_indices,
            **kwargs,
        ),
        val_generator=SirepoHDF5ImageGenerator(
            indices=s.validation_indices,
            **kwargs,
        ),
        test_generator=SirepoHDF5ImageGenerator(
            mode="test",
            indices=s.test_indices,
            **kwargs,
        )
    )


output_shape = {{ outputShape }}
split = image_train_val_test_split(
    src='{{ dataFile }}',
    X_key='images',
    y_key='metadata/image_types',
    labels_encoding=False,
    scaler=False,
    shuffle={{ shuffleEachEpoch }},
    {% if dataFile_inputsScaler != 'None' %}
    scale_fn_x=in_scaler_fn,
    {% endif %}
    {% if dataFile_outputsScaler != 'None' and dataFile_outputsScaler != dataFile_inputsScaler %}
    scale_fn_y=out_scaler_fn,
    {% elif dataFile_outputsScaler != 'None' and dataFile_outputsScaler == dataFile_inputsScaler %}
    scale_fn_y=in_scaler_fn,
    {% endif %}
    num_classes=output_shape,
    batch_size={{ neuralNet_batch_size }},
)
with h5py.File('{{ dataFile }}', "r", libver="latest", swmr=True) as file:
    testy = file[split.test_generator.y_key][split.test_generator._indices]
    input_shape = file["{{ inPath }}"].shape[1:]
    if len(input_shape) < 3:
        input_shape += (1,)
testx = split.test_generator