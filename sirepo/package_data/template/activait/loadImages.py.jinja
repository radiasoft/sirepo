from pykern.pkcollections import PKDict
from sklearn.model_selection import train_test_split
import h5py
import keras
import math
import numpy


class SirepoScaler:
    def __init__(
        self,
        values,
    ):
        self.columns = 1
        if len(values.shape) == 2:
            self.columns = values.shape[1]
        if self.columns > 1:
            self.domain = []
            for i in range(self.columns):
                self.domain.append(self.__compute_domain(values[:, i]))
        else:
            self.domain = self.__compute_domain(values)

    def scale(self, values):
        if self.columns == 1:
            return self.__scale(values, self.domain)
        values = values[:]
        for i in range(self.columns):
            values[:, i] = self.__scale(values[:, i], self.domain[i])
        return values

    def __compute_domain(self, values):
        # computes min, max, size
        d = [numpy.min(values), numpy.max(values)]
        d.append(d[1] - d[0])
        return d

    def __scale(self, values, domain):
        return (values - domain[0]) / domain[2]


class SirepoHDF5Sequence(keras.utils.Sequence):
    def __init__(
        self,
        filename,
        x,
        y,
        indices,
        shuffle,
        batch_size,
    ):
        self.filename = filename
        self.x = x
        self.y = y
        self.indices = indices
        self.batch_size = batch_size
        self.shuffle = shuffle
        self.return_value = None

    def set_return_value(self, name):
        self.return_value = name
        return self

    def __len__(self):
        return math.ceil(len(self.indices) / self.batch_size)

    def _scale(self, scaler, values):
        if not scaler:
            return values
        return scaler.scale(values)

    def __getitem__(self, idx):
        indices = numpy.sort(
            self.indices[idx * self.batch_size : (idx + 1) * self.batch_size]
        )
        with h5py.File(self.filename, "r", libver="latest", swmr=True) as f:
            x = f[self.x.key][indices]
            y = f[self.y.key][indices]
            if not x.any() and y.any():
                raise AssertionError(f"Empty batch: x={x} y={y}")
            if self.return_value:
                if self.return_value == "x":
                    return self._scale(self.x.scaler, x)
                elif self.return_value == "y":
                    return self._scale(self.y.scaler, y)
                raise AssertionError(f"invalid return_value specified: {return_value}")
            return self._scale(self.x.scaler, x), self._scale(self.y.scaler, y)

    def on_epoch_end(self):
        if self.shuffle and not self.return_value:
            numpy.random.shuffle(self.indices)


def _split_indices(indices_all):
    test_and_validate = (100 - {{partition_training}}) / 100
    validation_size = ((test_and_validate * 100) - {{partition_testing}}) / 100
    train_indices, tvx, _, tvy = train_test_split(
        indices_all,
        indices_all,
        test_size=test_and_validate,
        random_state=42,
        shuffle=True,
    )
    test_indices, val_indices, _, _ = train_test_split(
        tvx,
        tvy,
        test_size=validation_size / test_and_validate,
        random_state=42,
        shuffle=True,
    )
    return PKDict(
        train=train_indices,
        val=val_indices,
        test=test_indices,
    )


def image_train_val_test_split(
    filename,
    xkey,
    ykey,
    shuffle,
    batch_size,
    output_shape,
):
    with h5py.File(filename, "r") as f:
        indices_all = numpy.arange(f[xkey].shape[0])
        xs = SirepoScaler(f[xkey])
        ys = SirepoScaler(f[ykey])
    s = _split_indices(indices_all)
    r = PKDict()
    for k in s:
        r[k + "_gen"] = SirepoHDF5Sequence(
            filename=filename,
            x=PKDict(
                scaler=xs,
                key=xkey,
            ),
            y=PKDict(
                scaler=ys,
                key=ykey,
            ),
            indices=s[k],
            batch_size=batch_size,
            shuffle=shuffle,
        )
    return r


output_shape = {{outputShape}}
split = image_train_val_test_split(
    "{{ dataFile }}",
    "{{ inPath }}",
    "{{ outPath }}",
    {{shuffleEachEpoch}},
    {{neuralNet_batch_size}},
    output_shape,
)
with h5py.File("{{ dataFile }}", "r") as f:
    input_shape = f["{{ inPath }}"].shape[1:]
    if len(input_shape) < 3:
        input_shape += (1,)

testy = []
for y in split.test_gen.set_return_value("y"):
    testy += y.tolist()
testx = split.test_gen.set_return_value("x")
