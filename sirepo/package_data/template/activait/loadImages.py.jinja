import h5py
import numpy
from sklearn.model_selection import train_test_split
from pykern.pkcollections import PKDict
from h5imagegenerator import HDF5ImageGenerator


def _scale(values):
    # TODO(pjm): use selected input/output scalar on dataFile model
    return 2 * (values - numpy.min(values)) / (numpy.max(values) - numpy.min(values)) - 1.0


with h5py.File("{{ dataFile }}", "r") as f:
    test_and_validate = (100 - {{partition_training}}) / 100
    validation_size = ((test_and_validate * 100) - {{partition_testing}}) / 100
    x_values = f["{{ inPath }}"]
    y_values = f["{{ outPath }}"]
    {% if inScaling %}
    x_values = _scale(x_values)
    {% endif %}
    {% if outScaling %}
    y_values = _scale(y_values)
    {% endif %}

    trainx, tvx, trainy, tvy = train_test_split(
        x_values, y_values, test_size=test_and_validate, random_state=42, shuffle=False
    )
    testx, valx, testy, valy = train_test_split(
        tvx, tvy, test_size=validation_size, random_state=42, shuffle=False
    )
    input_shape = trainx.shape[1:]
    if len(input_shape) < 3:
        input_shape += (1,)
    print("input shape = ", input_shape)
    output_shape = {{outputShape}}

class SirepoHDF5ImageGenerator(HDF5ImageGenerator):
    def __init__(
        self,
        indices=None,
        *args,
        **kwargs,
    ):
        super().__init__(*args, **kwargs)
        if indices is not None:
            self._indices = indices


def _split_indices(indices_all):
    test_and_validate = (100 - {{partition_training}}) / 100
    validation_size = ((test_and_validate * 100) - {{partition_testing}}) / 100

    print("partitioning: ", test_and_validate, validation_size)
    print("val partitioning", validation_size/test_and_validate)
    numpy.random.shuffle(indices_all)
    train_indices, tvx, _, tvy = train_test_split(
        indices_all, indices_all, test_size=test_and_validate, random_state=42, shuffle=False
    )
    print("input indices shape: ", train_indices.shape[1:])
    print("len tvx", len(tvx))
    test_indices, val_indices, _, _ = train_test_split(
        tvx, tvy, test_size=validation_size/test_and_validate, random_state=42, shuffle=False
    )
    print("len train", len(train_indices))
    print("len test", len(test_indices))
    print("len val", len(val_indices))
    return PKDict(
        train_indices=train_indices,
        validation_indices=val_indices,
        test_indices=test_indices,
    )


def image_train_val_test_split(
    **kwargs
):
    print("kwargs:", kwargs)
    with h5py.File(kwargs.get("src"), "r", libver="latest", swmr=True) as file:
        indices_all = numpy.arange(file[kwargs.get("X_key")].shape[0])
    s = _split_indices(indices_all)

    return PKDict(
        train_generator=SirepoHDF5ImageGenerator(
            indices=s.train_indices,
            **kwargs,
        ),
        val_generator=SirepoHDF5ImageGenerator(
            indices=s.validation_indices,
            **kwargs,
        ),
        test_generator=SirepoHDF5ImageGenerator(
            mode="test",
            indices=s.test_indices,
            **kwargs,
        )
    )

split = image_train_val_test_split(
    src='{{ dataFile }}',
    X_key='images',
    y_key='metadata/image_types',
    labels_encoding=False,
    scaler=False,
    num_classes=output_shape,
    batch_size={{ neuralNet_batch_size }},
)
print(split.train_generator._indices)
print(split.val_generator._indices)
print(split.test_generator._indices)